{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####All Packages#####\n",
    "\n",
    "import math\n",
    "import numpy as np,numpy.linalg\n",
    "import pandas as pd\n",
    "from scipy.linalg import cholesky\n",
    "import random\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import OPTICS\n",
    "from functools import partial\n",
    "from fancyimpute import MatrixFactorization\n",
    "from scipy.stats import truncnorm\n",
    "import csv\n",
    "\n",
    "#####All functions#####\n",
    "\n",
    "##to get a positive semidefinite correlation matrix\n",
    "def _getAplus(A):\n",
    "    eigval, eigvec = np.linalg.eig(A)\n",
    "    Q = np.matrix(eigvec)\n",
    "    xdiag = np.matrix(np.diag(np.maximum(eigval, 0)))\n",
    "    return Q*xdiag*Q.T\n",
    "\n",
    "def _getPs(A, W=None):\n",
    "    W05 = np.matrix(W**.5)\n",
    "    return  W05.I * _getAplus(W05 * A * W05) * W05.I\n",
    "\n",
    "def _getPu(A, W=None):\n",
    "    Aret = np.array(A.copy())\n",
    "    Aret[W > 0] = np.array(W)[W > 0]\n",
    "    return np.matrix(Aret)\n",
    "\n",
    "def nearPD(A, nit=10):\n",
    "    n = A.shape[0]\n",
    "    W = np.identity(n) \n",
    "# W is the matrix used for the norm (assumed to be Identity matrix here)\n",
    "    deltaS = 0\n",
    "    Yk = A.copy()\n",
    "    for k in range(nit):\n",
    "        Rk = Yk - deltaS\n",
    "        Xk = _getPs(Rk, W=W)\n",
    "        deltaS = Xk - Rk\n",
    "        Yk = _getPu(Xk, W=W)\n",
    "    return Yk\n",
    "\n",
    "\n",
    "##Computing truncated gaussian numbers\n",
    "def get_truncated_normal(mean=0, sd=1, low=0, upp=10):\n",
    "    return truncnorm(\n",
    "        (low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd)\n",
    "\n"
   ]
  },
,
   "source": [
    "####Simulation of the dataset####\n",
    "##procedure for normal distributed values##\n",
    "\n",
    "seed = 30\n",
    "sample_size = 5000\n",
    "\n",
    "\n",
    "#A1 is for the dispersion of the distribution (from low to high)\n",
    "for A1 in range(0,3):\n",
    "\n",
    "    #C1 is for the correlation (from low to high)\n",
    "    for C1 in range(0,3):\n",
    "\n",
    "        #for each city one separate sample\n",
    "        for B1 in range(0,4):\n",
    "\n",
    "            #first step: generate gaussian random values with borders\n",
    "            X1 = get_truncated_normal(mean=2, sd=1*(1+A1), low=1, upp=3)\n",
    "            #second step: scale the random values to mean=0, std=1 in order to apply the correlation matrix\n",
    "            np.random.seed(seed+A1+C1+B1+1)\n",
    "            rnd1 = (X1.rvs(sample_size)-np.mean(X1.rvs(sample_size)))/np.std(X1.rvs(sample_size))\n",
    "            X2 = get_truncated_normal(mean=4, sd=2*(1+A1), low=3, upp=5)\n",
    "            np.random.seed(seed+A1+C1+B1+2)\n",
    "            rnd2 = (X2.rvs(sample_size)-np.mean(X2.rvs(sample_size)))/np.std(X2.rvs(sample_size))\n",
    "            X3 = get_truncated_normal(mean=1500-B1*125, sd=(750-B1*62.5)*(1+A1), low=500, upp=2500-B1*250)\n",
    "            np.random.seed(seed+A1+C1+B1+3)\n",
    "            rnd3 = (X3.rvs(sample_size)-np.mean(X3.rvs(sample_size)))/np.std(X3.rvs(sample_size))\n",
    "            X4 = get_truncated_normal(mean=3500-B1*250, sd=(1750-B1*125)*(1+A1), low=2500-B1*250, upp=4500-B1*250)\n",
    "            np.random.seed(seed+A1+C1+B1+4)\n",
    "            rnd4 = (X4.rvs(sample_size)-np.mean(X4.rvs(sample_size)))/np.std(X4.rvs(sample_size))\n",
    "\n",
    "            rnd = np.concatenate((np.reshape(rnd1,(sample_size,1)), np.reshape(rnd2,(sample_size,1)), np.reshape(rnd3,(sample_size,1)), np.reshape(rnd4,(sample_size,1))), axis=1)\n",
    "\n",
    "            #Correlations\n",
    "            corr = 0.25*(C1+1)\n",
    "            r1_2 = corr\n",
    "            r1_3 = corr\n",
    "            r1_4 = corr\n",
    "            r2_3 = corr\n",
    "            r2_4 = corr\n",
    "            r3_4 = corr\n",
    "\n",
    "            corr_mat = np.array([[1.0, r1_2, r1_3, r1_4],\n",
    "                                [r1_2, 1.0, r2_3, r2_4],\n",
    "                                [r1_3, r2_3, 1.0, r3_4],\n",
    "                                [r1_4, r2_4, r3_4, 1.0]])\n",
    "\n",
    "            #nearest positive semidefinite correlation matrix (if the chosen correlations do not lead to a semidefinite matrix)\n",
    "            corr_mat1 = nearPD(corr_mat,nit=10)\n",
    "\n",
    "            #Compute the (upper) Cholesky decomposition matrix\n",
    "            upper_chol = cholesky(corr_mat1)\n",
    "            ans = rnd @ upper_chol\n",
    "\n",
    "            #reshape data\n",
    "            np.random.seed(seed+A1+C1+B1+1)\n",
    "            ans[:,0]=ans[:,0]*np.std(X1.rvs(sample_size))+np.mean(X1.rvs(sample_size))\n",
    "            np.random.seed(seed+A1+C1+B1+2)\n",
    "            ans[:,1]=ans[:,1]*np.std(X2.rvs(sample_size))+np.mean(X2.rvs(sample_size))\n",
    "            np.random.seed(seed+A1+C1+B1+3)\n",
    "            ans[:,2]=ans[:,2]*np.std(X3.rvs(sample_size))+np.mean(X3.rvs(sample_size))\n",
    "            np.random.seed(seed+A1+C1+B1+4)\n",
    "            ans[:,3]=ans[:,3]*np.std(X4.rvs(sample_size))+np.mean(X4.rvs(sample_size))\n",
    "\n",
    "            #city 1\n",
    "            if B1 == 0:\n",
    "                lat = np.full((sample_size, 1), 47.37)\n",
    "                long = np.full((sample_size, 1), 8.54)\n",
    "                full = np.concatenate((np.rint(ans), lat, long), axis = 1)   \n",
    "\n",
    "            #city 2                                \n",
    "            if B1 == 1:\n",
    "                lat = np.full((sample_size, 1), 46.20)\n",
    "                long = np.full((sample_size, 1), 6.14)           \n",
    "                full1 = np.concatenate((np.rint(ans), lat, long), axis = 1)\n",
    "\n",
    "            #city 3                \n",
    "            if B1 == 2:\n",
    "                lat = np.full((sample_size, 1), 46.95)\n",
    "                long = np.full((sample_size, 1), 7.44)\n",
    "                full2 = np.concatenate((np.rint(ans), lat, long), axis = 1)\n",
    "\n",
    "            #city 4                \n",
    "            if B1 == 3:\n",
    "                lat = np.full((sample_size, 1), 47.56)\n",
    "                long = np.full((sample_size, 1), 7.59)                  \n",
    "                full3 = np.concatenate((np.rint(ans), lat, long), axis = 1)                \n",
    "\n",
    "                #putting all cities together\n",
    "                dataset_unscaled = np.concatenate((full, full1, full2, full3), axis = 0)\n",
    "                #scales data\n",
    "                dataset = (dataset_unscaled-np.mean(dataset_unscaled, axis=0))/np.std(dataset_unscaled, axis=0)\n",
    "                df = pd.DataFrame(data = dataset,\n",
    "                            columns = ['roomMin', 'roomMax', 'priceMin', 'priceMax', 'lat', 'long'])\n",
    "\n",
    "\n",
    "        #####Modifying the complete simulated dataset#####\n",
    "\n",
    "        #D1 is for the missingness\n",
    "        for D1 in range(0,3):\n",
    "\n",
    "            #E1 is for the missingness design (0=MCAR, 1=MAR)\n",
    "            for E1 in range(0,2):\n",
    "\n",
    "                df_NaN_empty = pd.DataFrame()\n",
    "                #F1 is for each city\n",
    "                for F1 in range(0,4):                   \n",
    "\n",
    "                    #percentage missing\n",
    "                    #a for roomMin, b for roomMax, c for priceMin, d for priceMax\n",
    "                    a = 0.05*(1+D1)*0.5+0.1*(-E1)**(F1+2)\n",
    "                    b = 0.20*(1+D1)*0.5+0.15*(-E1)**(F1+1)\n",
    "                    c = 0.30*(1+D1)*0.5+0.16*(-E1)**(F1+2)\n",
    "                    d = 0.02*(1+D1)*0.5+0.05*(-E1)**(F1+1)\n",
    "\n",
    "                    #create the 'missingness matrix'\n",
    "                    np.random.seed(seed+A1+C1+B1+D1+E1+5)\n",
    "                    df_NaN = pd.DataFrame({'NaN1' : np.random.sample(sample_size),\n",
    "                                           'NaN2' : np.random.sample(sample_size),\n",
    "                                           'NaN3' : np.random.sample(sample_size),\n",
    "                                           'NaN4' : np.random.sample(sample_size),\n",
    "                                           'NaN5' : np.full(sample_size,1),\n",
    "                                           'NaN6' : np.full(sample_size,1)})\n",
    "\n",
    "                    #transform the random values (uniform dist from 0 to 1) into integers referring to the missingness fraction\n",
    "                    df_NaN.loc[(df_NaN.NaN1 <= a), 'NaN1'] = 0\n",
    "                    df_NaN.loc[(df_NaN.NaN1 > a), 'NaN1'] = 1\n",
    "\n",
    "                    df_NaN.loc[(df_NaN.NaN2 <= b), 'NaN2'] = 0\n",
    "                    df_NaN.loc[(df_NaN.NaN2 > b), 'NaN2'] = 1\n",
    "\n",
    "                    df_NaN.loc[(df_NaN.NaN3 <= c), 'NaN3'] = 0\n",
    "                    df_NaN.loc[(df_NaN.NaN3 > c), 'NaN3'] = 1\n",
    "\n",
    "                    df_NaN.loc[(df_NaN.NaN4 <= d), 'NaN4'] = 0\n",
    "                    df_NaN.loc[(df_NaN.NaN4 > d), 'NaN4'] = 1\n",
    "\n",
    "                    df_NaN_empty = df_NaN_empty.append(df_NaN, ignore_index = True)\n",
    "\n",
    "\n",
    "                #multiply the missingness matrix with the dataset\n",
    "                df_with_missing = pd.DataFrame(df.values*df_NaN_empty.values, columns=df.columns, index=df.index)\n",
    "\n",
    "                #remove rows where roomMin and roomMax or priceMin and priceMax are missing\n",
    "                Z = np.where((df_with_missing[df_with_missing.columns[0]] + df_with_missing[df_with_missing.columns[1]]==0) | (df_with_missing[df_with_missing.columns[2]] + df_with_missing[df_with_missing.columns[3]]==0))\n",
    "                df_new = df_with_missing.drop(Z[0])\n",
    "\n",
    "                #transform the 0 into NaN\n",
    "                df_new.loc[(df_new.roomMin == 0),'roomMin']= np.nan\n",
    "                df_new.loc[(df_new.roomMax == 0),'roomMax']= np.nan\n",
    "                df_new.loc[(df_new.priceMin == 0),'priceMin']= np.nan\n",
    "                df_new.loc[(df_new.priceMax == 0),'priceMax']= np.nan            \n",
    "\n",
    "\n",
    "                #####Preparing data for performance testing and iterative imputation#####\n",
    "\n",
    "                np.random.seed(seed+A1+C1+B1+D1+E1+6)\n",
    "                #splitting the dataset into two subsets\n",
    "                #df1 is for final performance testing (10% of the whole dataset) - the hold-out dataset\n",
    "                #df2 is for training and testing (90% of the whole dataset)\n",
    "                msk1 = np.random.rand(len(df_new)) < 0.1\n",
    "                df1 = df_new[msk1]\n",
    "                df2 = df_new[~msk1]\n",
    "\n",
    "                #removing all rows with missing values in df2 (D0 in paper)\n",
    "                df2_full = df2.dropna()\n",
    "                np.random.seed(seed+A1+C1+B1+D1+E1+7)\n",
    "                #splitting into test (df2_test, D0,2 in paper) (20%) and training (df2_train, D0,1 in paper) (80%) set\n",
    "                msk2 = np.random.rand(len(df2_full)) < 0.8\n",
    "                df2_train = df2_full[msk2]\n",
    "                df2_test = df2_full[~msk2]\n",
    "                np.random.seed(seed+A1+C1+B1+D1+E1+8)\n",
    "                #second missingness matrix to determine which values from the test dataset (D0,2) to remove \n",
    "                #to have the same missingness pattern\n",
    "                df_NaN2 = pd.DataFrame({'NaN1' : np.random.sample(len(df2_test)),\n",
    "                                        'NaN2' : np.random.sample(len(df2_test)),\n",
    "                                        'NaN3' : np.random.sample(len(df2_test)),\n",
    "                                        'NaN4' : np.random.sample(len(df2_test)),\n",
    "                                        'NaN5' : np.full(len(df2_test),1),\n",
    "                                        'NaN6' : np.full(len(df2_test),1)})\n",
    "\n",
    "                E1 = 0\n",
    "\n",
    "                df_NaN2.loc[(df_NaN2.NaN1 <= a), 'NaN1'] = 0\n",
    "                df_NaN2.loc[(df_NaN2.NaN1 > a), 'NaN1'] = 1\n",
    "\n",
    "                df_NaN2.loc[(df_NaN2.NaN2 <= b), 'NaN2'] = 0\n",
    "                df_NaN2.loc[(df_NaN2.NaN2 > b), 'NaN2'] = 1\n",
    "\n",
    "                df_NaN2.loc[(df_NaN2.NaN3 <= c), 'NaN3'] = 0\n",
    "                df_NaN2.loc[(df_NaN2.NaN3 > c), 'NaN3'] = 1\n",
    "\n",
    "                df_NaN2.loc[(df_NaN2.NaN4 <= d), 'NaN4'] = 0\n",
    "                df_NaN2.loc[(df_NaN2.NaN4 > d), 'NaN4'] = 1\n",
    "\n",
    "                #multiply the missingness matrix with the dataset\n",
    "                #df2_test_v1 (D0,2 mod in paper) is the test data set with the same missingness pattern as the original dataset (D in paper)\n",
    "                df2_test_v1 = pd.DataFrame(df2_test.values*df_NaN2.values, columns=df2_test.columns, index=df2_test.index)\n",
    "\n",
    "\n",
    "                #remove rows where roomMin and roomMax or priceMin and priceMax are missing and keeping the index\n",
    "                Y = df2_test_v1[(df2_test_v1[df2_test_v1.columns[0]] + df2_test_v1[df2_test_v1.columns[1]]==0) | (df2_test_v1[df2_test_v1.columns[2]] + df2_test_v1[df2_test_v1.columns[3]]==0)]\n",
    "                df2_test_new = df2_test_v1.drop(Y.index)\n",
    "\n",
    "                #transform the 0 into NaN\n",
    "                df2_test_new.loc[(df2_test_new.roomMin == 0),'roomMin']= np.nan\n",
    "                df2_test_new.loc[(df2_test_new.roomMax == 0),'roomMax']= np.nan\n",
    "                df2_test_new.loc[(df2_test_new.priceMin == 0),'priceMin']= np.nan\n",
    "                df2_test_new.loc[(df2_test_new.priceMax == 0),'priceMax']= np.nan\n",
    "\n",
    "                ##at the beginning, there are already complete rows in the test set\n",
    "                ##those rows have to added to the train set   \n",
    "\n",
    "                df2_test_new = df2_test_new[df2_test_new.isnull().sum(axis=1)!=0]\n",
    "                df2_train = pd.concat([df2_train,df2_test_new[df2_test_new.isnull().sum(axis=1)==0]])\n",
    "                df2_train_reset = pd.concat([df2_train,df2_test_new[df2_test_new.isnull().sum(axis=1)==0]])\n",
    "\n",
    "                #combination of df2_train and df2_test_new (for later performance testing)\n",
    "                df2_full1 = pd.concat([df2_train,df2_test_new])\n",
    "                df2_full_comp = df2_full.loc[df2_full1.index]\n",
    "\n"
   ]
  },
   "source": [
    "####Simulation of the dataset####\n",
    "##procedure for chi squared distributed values##\n",
    "\n",
    "seed = 1900\n",
    "sample_size = 5000\n",
    "\n",
    "#A1 is for the dispersion of the distribution (from low to high)\n",
    "for A1 in range(0,3):   \n",
    "\n",
    "    #C1 is for the correlation (from low to high)\n",
    "    for C1 in range(0,3):   \n",
    "\n",
    "        #for each city one separate sample\n",
    "        for B1 in range(0,4):\n",
    "            np.random.seed(seed+A1+C1+B1)\n",
    "            rnd = np.random.chisquare(3, size=(sample_size*4,4))\n",
    "\n",
    "            if C1 == 0:\n",
    "            #Correlations\n",
    "                r1_2 = 0.1\n",
    "                r1_3 = 0.2\n",
    "                r1_4 = 0.2\n",
    "                r2_3 = 0.10\n",
    "                r2_4 = 0.20\n",
    "                r3_4 = 0.15\n",
    "\n",
    "            if C1 == 1:\n",
    "            #Correlations\n",
    "                r1_2 = 0.55\n",
    "                r1_3 = 0.5\n",
    "                r1_4 = 0.55\n",
    "                r2_3 = 0.45\n",
    "                r2_4 = 0.55\n",
    "                r3_4 = 0.45                  \n",
    "\n",
    "            if C1 == 2:\n",
    "            #Correlations\n",
    "                r1_2 = 0.8\n",
    "                r1_3 = 0.75\n",
    "                r1_4 = 0.8\n",
    "                r2_3 = 0.75\n",
    "                r2_4 = 0.75\n",
    "                r3_4 = 0.75\n",
    "\n",
    "\n",
    "            corr_mat = np.array([[1.0, r1_2, r1_3, r1_4],\n",
    "                                [r1_2, 1.0, r2_3, r2_4],\n",
    "                                [r1_3, r2_3, 1.0, r3_4],\n",
    "                                [r1_4, r2_4, r3_4, 1.0]])\n",
    "\n",
    "            #positive semidefinite correlation matrix\n",
    "            corr_mat1 = nearPD(corr_mat,nit=10)\n",
    "\n",
    "            #Compute the (upper) Cholesky decomposition matrix\n",
    "            upper_chol = cholesky(corr_mat1)\n",
    "            ans = rnd @ upper_chol\n",
    "\n",
    "            #first line scales the standard deviation\n",
    "            #second line scales the mean\n",
    "\n",
    "            ans[:,0] = (ans[:,0]/math.sqrt(6))*1*(A1+1)\n",
    "            ans[:,0] = ans[:,0]+(2-np.mean(ans[:,0]))\n",
    "\n",
    "            ans[:,1] = (ans[:,1]/math.sqrt(6))*2*(A1+1)\n",
    "            ans[:,1] = ans[:,1]+(4-np.mean(ans[:,1]))\n",
    "\n",
    "            ans[:,2] = (ans[:,2]/math.sqrt(6))*(750-B1*62.5)*(A1+1)\n",
    "            ans[:,2] = ans[:,2]+(1500-B1*125-np.mean(ans[:,2]))\n",
    "\n",
    "            ans[:,3] = (ans[:,3]/math.sqrt(6))*(1750-B1*125)*(A1+1)\n",
    "            ans[:,3] = ans[:,3]+(3500-B1*250-np.mean(ans[:,3]))\n",
    "\n",
    "            #removing rows where roomMin < 0.5\n",
    "            A2 = ans[np.where(ans[:,0]>= 0.5)]\n",
    "            #removing rows where roomMin > roomMax\n",
    "            A3 = A2[np.where(A2[:,0] <= A2[:,1])]\n",
    "            #removing rows where priceMin > priceMax\n",
    "            A4 = A3[np.where(A3[:,2] <= A3[:,3])]\n",
    "\n",
    "\n",
    "            #city 1\n",
    "            if B1 == 0:\n",
    "                lat = np.full((sample_size, 1), 47.37)\n",
    "                long = np.full((sample_size, 1), 8.54)\n",
    "                full = np.concatenate((np.rint(A4[:sample_size]), lat, long), axis = 1)   \n",
    "\n",
    "            #city 2                                \n",
    "            if B1 == 1:\n",
    "                lat = np.full((sample_size, 1), 46.20)\n",
    "                long = np.full((sample_size, 1), 6.14)           \n",
    "                full1 = np.concatenate((np.rint(A4[:sample_size]), lat, long), axis = 1)\n",
    "\n",
    "            #city 3                \n",
    "            if B1 == 2:\n",
    "                lat = np.full((sample_size, 1), 46.95)\n",
    "                long = np.full((sample_size, 1), 7.44)\n",
    "                full2 = np.concatenate((np.rint(A4[:sample_size]), lat, long), axis = 1)\n",
    "\n",
    "            #city 4                \n",
    "            if B1 == 3:\n",
    "                lat = np.full((sample_size, 1), 47.56)\n",
    "                long = np.full((sample_size, 1), 7.59)                  \n",
    "                full3 = np.concatenate((np.rint(A4[:sample_size]), lat, long), axis = 1)                \n",
    "\n",
    "                #putting all cities together\n",
    "                dataset_unscaled = np.concatenate((full, full1, full2, full3), axis = 0)\n",
    "                #scales data\n",
    "                dataset = (dataset_unscaled-np.mean(dataset_unscaled, axis=0))/np.std(dataset_unscaled, axis=0)\n",
    "                df = pd.DataFrame(data = dataset,\n",
    "                            columns = ['roomMin', 'roomMax', 'priceMin', 'priceMax', 'lat', 'long'])\n",
    "\n",
    "\n",
    "\n",
    "        #####Modifying the complete simulated dataset#####\n",
    "\n",
    "        #D1 is for the missingness (low to high)\n",
    "        for D1 in range(0,3):\n",
    "\n",
    "            #E1 is for the missingness design (0=MCAR, 1=MAR)\n",
    "            for E1 in range(0,2):\n",
    "\n",
    "                df_NaN_empty = pd.DataFrame()\n",
    "                #F1 is for each city\n",
    "                for F1 in range(0,4):                   \n",
    "\n",
    "                    #percentage missing\n",
    "                    #a for roomMin, b for roomMax, c for priceMin, d for priceMax\n",
    "                    a = 0.05*(1+D1)*0.5+0.1*(-E1)**(F1+2)\n",
    "                    b = 0.20*(1+D1)*0.5+0.15*(-E1)**(F1+1)\n",
    "                    c = 0.30*(1+D1)*0.5+0.16*(-E1)**(F1+2)\n",
    "                    d = 0.02*(1+D1)*0.5+0.05*(-E1)**(F1+1)\n",
    "\n",
    "                    np.random.seed(seed+A1+C1+B1+D1+E1+1)\n",
    "                    #create the 'missingness matrix'\n",
    "                    df_NaN = pd.DataFrame({'NaN1' : np.random.sample(sample_size),\n",
    "                                           'NaN2' : np.random.sample(sample_size),\n",
    "                                           'NaN3' : np.random.sample(sample_size),\n",
    "                                           'NaN4' : np.random.sample(sample_size),\n",
    "                                           'NaN5' : np.full(sample_size,1),\n",
    "                                           'NaN6' : np.full(sample_size,1)})\n",
    "\n",
    "                    #transform the random values (uniform dist from 0 to 1) into integers referring to the missingness fraction\n",
    "                    df_NaN.loc[(df_NaN.NaN1 <= a), 'NaN1'] = 0\n",
    "                    df_NaN.loc[(df_NaN.NaN1 > a), 'NaN1'] = 1\n",
    "\n",
    "                    df_NaN.loc[(df_NaN.NaN2 <= b), 'NaN2'] = 0\n",
    "                    df_NaN.loc[(df_NaN.NaN2 > b), 'NaN2'] = 1\n",
    "\n",
    "                    df_NaN.loc[(df_NaN.NaN3 <= c), 'NaN3'] = 0\n",
    "                    df_NaN.loc[(df_NaN.NaN3 > c), 'NaN3'] = 1\n",
    "\n",
    "                    df_NaN.loc[(df_NaN.NaN4 <= d), 'NaN4'] = 0\n",
    "                    df_NaN.loc[(df_NaN.NaN4 > d), 'NaN4'] = 1\n",
    "\n",
    "                    df_NaN_empty = df_NaN_empty.append(df_NaN, ignore_index = True)\n",
    "\n",
    "\n",
    "                #multiply the missingness matrix with the dataset\n",
    "                df_with_missing = pd.DataFrame(df.values*df_NaN_empty.values, columns=df.columns, index=df.index)\n",
    "\n",
    "                #remove rows where roomMin and roomMax or priceMin and priceMax are missing\n",
    "                Z = np.where((df_with_missing[df_with_missing.columns[0]] + df_with_missing[df_with_missing.columns[1]]==0) | (df_with_missing[df_with_missing.columns[2]] + df_with_missing[df_with_missing.columns[3]]==0))\n",
    "                df_new = df_with_missing.drop(Z[0])\n",
    "\n",
    "                #transform the 0 into NaN\n",
    "                df_new.loc[(df_new.roomMin == 0),'roomMin']= np.nan\n",
    "                df_new.loc[(df_new.roomMax == 0),'roomMax']= np.nan\n",
    "                df_new.loc[(df_new.priceMin == 0),'priceMin']= np.nan\n",
    "                df_new.loc[(df_new.priceMax == 0),'priceMax']= np.nan            \n",
    "\n",
    "\n",
    "                #####Preparing data for performance testing and iterative imputation#####\n",
    "\n",
    "                np.random.seed(seed+A1+C1+B1+D1+E1+2)\n",
    "                #splitting the dataset into two subsets\n",
    "                #df1 is for final performance testing (10% of the whole dataset) - the hold-out dataset\n",
    "                #df2 is for training and testing (90% of the whole dataset)\n",
    "                msk1 = np.random.rand(len(df_new)) < 0.1\n",
    "                df1 = df_new[msk1]\n",
    "                df2 = df_new[~msk1]\n",
    "\n",
    "                #removing all rows with missing values in df2 (D0 in paper)\n",
    "                df2_full = df2.dropna()\n",
    "                np.random.seed(seed+A1+C1+B1+D1+E1+3)\n",
    "                #splitting into test (df2_test, D0,2 in paper) (20%) and training (df2_train, D0,1 in paper) (80%) set\n",
    "                msk2 = np.random.rand(len(df2_full)) < 0.8\n",
    "                df2_train = df2_full[msk2]\n",
    "                df2_test = df2_full[~msk2]\n",
    "                np.random.seed(seed+A1+C1+B1+D1+E1+4)\n",
    "                #second missingness matrix to determine which values from the test dataset (D0,2) to remove \n",
    "                #to have the same missingness pattern\n",
    "                df_NaN2 = pd.DataFrame({'NaN1' : np.random.sample(len(df2_test)),\n",
    "                                        'NaN2' : np.random.sample(len(df2_test)),\n",
    "                                        'NaN3' : np.random.sample(len(df2_test)),\n",
    "                                        'NaN4' : np.random.sample(len(df2_test)),\n",
    "                                        'NaN5' : np.full(len(df2_test),1),\n",
    "                                        'NaN6' : np.full(len(df2_test),1)})\n",
    "\n",
    "                E1 = 0\n",
    "\n",
    "                df_NaN2.loc[(df_NaN2.NaN1 <= a), 'NaN1'] = 0\n",
    "                df_NaN2.loc[(df_NaN2.NaN1 > a), 'NaN1'] = 1\n",
    "\n",
    "                df_NaN2.loc[(df_NaN2.NaN2 <= b), 'NaN2'] = 0\n",
    "                df_NaN2.loc[(df_NaN2.NaN2 > b), 'NaN2'] = 1\n",
    "\n",
    "                df_NaN2.loc[(df_NaN2.NaN3 <= c), 'NaN3'] = 0\n",
    "                df_NaN2.loc[(df_NaN2.NaN3 > c), 'NaN3'] = 1\n",
    "\n",
    "                df_NaN2.loc[(df_NaN2.NaN4 <= d), 'NaN4'] = 0\n",
    "                df_NaN2.loc[(df_NaN2.NaN4 > d), 'NaN4'] = 1\n",
    "\n",
    "                #multiply the missingness matrix with the dataset\n",
    "                #df2_test_v1 (D0,2 mod in paper) is the test data set with the same missingness pattern as the original dataset (D in paper)\n",
    "                df2_test_v1 = pd.DataFrame(df2_test.values*df_NaN2.values, columns=df2_test.columns, index=df2_test.index)\n",
    "\n",
    "\n",
    "                #remove rows where roomMin and roomMax or priceMin and priceMax are missing and keeping the index\n",
    "                Y = df2_test_v1[(df2_test_v1[df2_test_v1.columns[0]] + df2_test_v1[df2_test_v1.columns[1]]==0) | (df2_test_v1[df2_test_v1.columns[2]] + df2_test_v1[df2_test_v1.columns[3]]==0)]\n",
    "                df2_test_new = df2_test_v1.drop(Y.index)\n",
    "\n",
    "                #transform the 0 into NaN\n",
    "                df2_test_new.loc[(df2_test_new.roomMin == 0),'roomMin']= np.nan\n",
    "                df2_test_new.loc[(df2_test_new.roomMax == 0),'roomMax']= np.nan\n",
    "                df2_test_new.loc[(df2_test_new.priceMin == 0),'priceMin']= np.nan\n",
    "                df2_test_new.loc[(df2_test_new.priceMax == 0),'priceMax']= np.nan\n",
    "\n",
    "                ##at the beginning, there are already complete rows in the test set\n",
    "                ##those rows have to added to the train set   \n",
    "\n",
    "                df2_test_new = df2_test_new[df2_test_new.isnull().sum(axis=1)!=0]\n",
    "                df2_train = pd.concat([df2_train,df2_test_new[df2_test_new.isnull().sum(axis=1)==0]])\n",
    "                df2_train_reset = pd.concat([df2_train,df2_test_new[df2_test_new.isnull().sum(axis=1)==0]])\n",
    "\n",
    "                #combination of df2_train and df2_test_new (for later performance testing)\n",
    "                df2_full1 = pd.concat([df2_train,df2_test_new])\n",
    "                df2_full_comp = df2_full.loc[df2_full1.index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
